{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "605b9f9b",
   "metadata": {},
   "source": [
    "# ADS-509 Assignment 3.1\n",
    "## Word Embeddings\n",
    "\n",
    "In this assignment, you will use the HackerNews dataset created in the Module 1 assignment o:\n",
    "- Build static embeddings by training Word2Vec\n",
    "- Build contextual embeddings with a sentence-transformer (BERT-family)\n",
    "- Perform EDA on embeddings  \n",
    "- Train a simple classifier (logistic regression) on each embedding type to predict a lightweight label from story titles\n",
    "\n",
    "If you are not confident in the quality of your own dataset from Module 1, there is a clean dataset available for your use in Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfbcd1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it.\n",
    "\n",
    "Work through this notebook as if it were a worksheet, completing the code sections marked with **TODO** in the cells provided. Similarly, written questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you to fill in with your answers. **Make sure to answer every question marked with a Q: for full credit**.\n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential import statements and make sure that all such statements are moved into the designated cell.\n",
    "\n",
    "A .pdf of this notebook, with your completed code and written answers, is what you should submit in Canvas for full credit. **DO NOT SUBMIT A NEW NOTEBOOK FILE OR A RAW .PY FILE**. Submitting in a different format makes it difficult to grade your work, and students who have done this in the past inevitably miss some of the required work or written questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6cfa7",
   "metadata": {},
   "source": [
    "## Imports and Downloads\n",
    "\n",
    "Once again we will use some datasets from the NLTK library, so we need to make sure that these are downloaded (you should have them from Module 2, but it never hurts to double check).\n",
    "\n",
    "We will also be using the pre-trained embedding models (Word2Vec and SentenceTransformer) from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef806cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string, random, math, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Embeddings\n",
    "from gensim.models import Word2Vec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 120\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838b5e7-4dc3-4acb-a457-7abeae9b3799",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Next we will load our dataset from Module 2 and double check that it is formatted correctly.\n",
    "\n",
    "If you are uncertain about your own dataset, or if you don't pass the check below, feel free to use the dataset provided on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca68d96-4755-4524-96d5-96dfb5256389",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/module2/hn_comment_features.csv'  # TODO: Update the file path as needed\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Dataset not found at {DATA_PATH}. Update the path for your environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfda6ea-be4b-426c-a1c6-ffc2af01d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Rows:', len(df))\n",
    "expected_cols = {'story_id','title','comment_id','user','comment_text','text_norm','tokens_clean','sent_compound'}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    print('Warning: missing expected columns:', missing)\n",
    "\n",
    "df[\"tokens_clean\"] = df[\"tokens_clean\"].apply(ast.literal_eval) # convert python list from a string\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff62ca4",
   "metadata": {},
   "source": [
    "## Create a label for classification\n",
    "\n",
    "The dataset that we scraped from HackerNews doesn't have an obvious variable for use in a classification task (which we will need later). There are many ways that we could create such a label, but here we will use string matching to create some rough labels based on the words used in the article titles.\n",
    "\n",
    "**Q**: Give at least two other ideas for how we could label our dataset for classification. Keep data balance in mind (i.e. the resulting dataset should not be completely dominated by one label) in a brief discussion of why that method would/wouldn't be a good choice.\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c575eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple label from the story title\n",
    "def label_from_title(title):\n",
    "    if not isinstance(title, str):\n",
    "        return None\n",
    "    t = title.strip().lower()\n",
    "    if any(kw in t for kw in ['rust', 'python', 'sql', 'linux', 'windows', 'ios', 'c++', 'perl', 'pfp', 'java']):\n",
    "        return 'programming'\n",
    "    if any(kw in t for kw in ['google', 'facebook', 'meta', 'apple', 'amazon', 'microsoft']):\n",
    "        return 'big-tech'\n",
    "    if any(kw in t for kw in ['ai ', 'torch', 'llm', 'large language model', 'claude', 'gemini', 'copilot']):\n",
    "        return 'ai'\n",
    "    else:\n",
    "        return 'other'\n",
    "    return None\n",
    "\n",
    "df['label'] = df['title'].apply(label_from_title)\n",
    "\n",
    "print('Total rows for task:', len(df))\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "df[['title','comment_text','label']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb26e7d",
   "metadata": {},
   "source": [
    "## Train Static Word Embedding Model\n",
    "\n",
    "We will train a Word2Vec model to produce static word embeddings for our normalized text, which we will use later for data exploration and classification.\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "Use the gensim Word2Vec class to train a static embedding model on our tokenized comment text. Check out the [documentation](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html#gensim-models-word2vec) to assign the following settings (Hint: You might need to go into the class source code to find argument descriptions):\n",
    "\n",
    "- Embedding size 100\n",
    "- Sequence window size 5\n",
    "- Limit to tokens that appear at least 3 times\n",
    "- Skip-gram algorithm (rather than CBOW)\n",
    "- Run training for 10 epochs\n",
    "- If you have multiple CPUs available, set the number of workers to improve training speed\n",
    "\n",
    "**Q**: Why do we call a model like Word2Vec a *static* word embedding model?\n",
    "\n",
    "**A**: \n",
    "\n",
    "**Q**: What is the difference between the CBOW and skip-gram algorithms?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deffad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train a Word2Vec model on our normalized text\n",
    "sentences = ??\n",
    "\n",
    "w2v = ??\n",
    "\n",
    "w2v_vecs = w2v.wv\n",
    "len(w2v_vecs), w2v_vecs.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7bf93-7199-4a3e-bc3c-02868d170f05",
   "metadata": {},
   "source": [
    "The word embeddings that we just created can now be used to perform a semantic comparison of individual words/tokens in our vocabulary.\n",
    "\n",
    "**TODO**:\n",
    "- Choose a few words from our corpus vocabulary and print the 5 nearest neighbors using the Word2Vec.most_similar() function\n",
    "\n",
    "**Q**: Are the nearest neighbors semantically similar to your chosen words? Think about how the Word2Vec model works and provide a 2-3 sentence description of why this comparison does/doesn't work for our dataset.\n",
    "\n",
    "**A**: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a94f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: choose a few words from our vocabulary and examine their 5 nearest neighbors\n",
    "\n",
    "probe_terms = [??]\n",
    "for term in probe_terms:\n",
    "    ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8e9c69",
   "metadata": {},
   "source": [
    "## Build Document Embeddings \n",
    "\n",
    "One way to represent a document (comment) in our dataset is to aggregate all of the individual word embeddings by computing an average embedding or document vector. This vector can then be used to compare entire documents instead of individual words.\n",
    "\n",
    "**TODO**:\n",
    "- Average the word/token embeddings for each comment in our dataset to produce a single document embedding vector\n",
    "- Compute the cosine similarity between the first comment's document embedding and all of the rest\n",
    "- Print out the two most and least similar comments\n",
    "\n",
    "**Q**: Describe your results. Does the cosine similarity of document/comment embeddings do a good job of identifying similar content?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: average the word/token embeddings for each comment\n",
    "\n",
    "def docvec_average(tokens, wv):\n",
    "    vecs = [wv[t] for t in tokens if t in wv]\n",
    "    if not vecs:\n",
    "        return np.zeros(wv.vector_size, dtype=np.float32)\n",
    "    return ??\n",
    "\n",
    "df[\"w2v_embedding\"] = list(np.vstack([docvec_average(toks, w2v_vecs) for toks in df['tokens_clean']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4ec5d-65f1-4a9a-9583-3b0bc7ecf9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute the cosine similarity between the first comment and the rest of the dataset\n",
    "\n",
    "def docvec_similarity(target, docvecs):\n",
    "    sims = ??\n",
    "    return sims\n",
    "\n",
    "df[\"w2v_sim\"] = [None] + docvec_similarity(df[\"w2v_embedding\"][0], df[\"w2v_embedding\"][1:]) # create new column with None for self-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b21cf3-f16c-446f-8647-aab8580643db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print out the target comment and the two comments with the highest and lowest cosine similarities\n",
    "\n",
    "print(\"Target Comment:\")\n",
    "print(df[\"comment_text\"][0])\n",
    "\n",
    "print(\"\\nLowest Similarities:\")\n",
    "print(??)\n",
    "\n",
    "print(\"\\nHighest similarities:\")\n",
    "print(??)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82b106e",
   "metadata": {},
   "source": [
    "## Build Contextual Embeddings with Sentence-Transformers\n",
    "\n",
    "Another way to create a document-level embedding is to use a more sophisticated, pre-trained embedding model, like a Sentence Transformer (based on the BERT family architecture). These models use an *attention block* to create context-specific embeddings for a string of text, rather than simply averaging the static word embeddings as we did above.\n",
    "\n",
    "**TODO**:\n",
    "- Use the gensim SentenceTransformer class to load the  pre-trained 'sentence-transformers/all-MiniLM-L6-v2' model\n",
    "- Use this model to create embeddings for our comment text (HINT: You don't need to normalize or tokenize text before feeding it into a transformer model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the gensim pre-trained model to create embeddings for our comment text\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "st_model = SentenceTransformer(model_name)\n",
    "\n",
    "df[\"bert_embedding\"]= ??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd6011-3784-4f89-a01d-bfb471827ec3",
   "metadata": {},
   "source": [
    "Now we will compute the same cosine similarities as we did with the Word2Vec embeddings above.\n",
    "\n",
    "**Q**: How do the transformer-based embedding similarities compare to what you found above? Which method would you choose and why?\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3337e-4931-4da5-ac79-baa07827fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bert_sim\"] = [None] + docvec_similarity(df[\"bert_embedding\"][0], df[\"bert_embedding\"][1:]) # create new column with None for self-similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27adec48-192e-4854-8d80-fe077466dd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print out the target comment and the two comments with the highest and lowest cosine similarities\n",
    "\n",
    "print(\"Target Comment:\")\n",
    "print(df[\"comment_text\"][0])\n",
    "\n",
    "print(\"\\nLowest Similarities:\")\n",
    "print(??)\n",
    "\n",
    "print(\"\\nHighest similarities:\")\n",
    "print(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccf2e2",
   "metadata": {},
   "source": [
    "## Embedding-based classification\n",
    "\n",
    "Finally, we will explore how well the document embeddings perform on the classification task of predicting our comment category labels. We will use the two types of embeddings for input to two different logistic regression models, and then compare the performance.\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "- Use the sklearn LogisticRegression class to predict our comment labels using both embedding methods\n",
    "\n",
    "**Q**: How do the two embedding methods perform? Use the results from the model metrics and the confusion matrices to discuss and compare.\n",
    "\n",
    "**A**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train/test split (same split for both)\n",
    "Xw_train, Xw_test, y_train, y_test = train_test_split(df['w2v_embedding'], df['label'], test_size=0.2, random_state=42, stratify=df['label'])\n",
    "Xb_train, Xb_test, _, _ = train_test_split(df['bert_embedding'], df['label'], test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Standardize \n",
    "sc_w2v = StandardScaler(with_mean=False)\n",
    "Xw_train_s = sc_w2v.fit_transform(np.stack(Xw_train.to_numpy()))\n",
    "Xw_test_s  = sc_w2v.transform(np.stack(Xw_test.to_numpy()))\n",
    "\n",
    "sc_bert = StandardScaler(with_mean=False)\n",
    "Xb_train_s = sc_bert.fit_transform(np.stack(Xb_train.to_numpy()))\n",
    "Xb_test_s  = sc_bert.transform(np.stack(Xb_test.to_numpy()))\n",
    "\n",
    "# TODO: Train logistic regression models with both embedding methods\n",
    "clf_w2v = ??\n",
    "pred_w2v = ??\n",
    "\n",
    "clf_bert = ??\n",
    "pred_bert = ??\n",
    "\n",
    "# Print logistic regression model performance metrics\n",
    "print(\"\\n=== Word2Vec Avg → Logistic Regression ===\\n\")\n",
    "print(classification_report(y_test, pred_w2v, digits=3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred_w2v))\n",
    "\n",
    "print(\"\\n=== Sentence-BERT → Logistic Regression ===\\n\")\n",
    "print(classification_report(y_test, pred_bert, digits=3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred_bert))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
